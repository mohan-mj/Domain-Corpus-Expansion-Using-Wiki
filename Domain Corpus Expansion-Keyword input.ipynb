{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import wikipedia\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: jmo4cob\n",
      "Password: ········\n"
     ]
    }
   ],
   "source": [
    "user = input('Username: ')\n",
    "passwd = getpass.getpass(\"Password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path (Eg. C:\\Users\\....\\keywords.txt):keywords.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = input(r\"File Path (Eg. C:\\Users\\....\\keywords.txt):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "httpProxy = \"http://\" + user + \":\" + passwd + \"@rb-proxy-de02.bosch.com:8080\"\n",
    "httpsProxy = \"https://\" + user + \":\" + passwd + \"@rb-proxy-de02.bosch.com:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['http_proxy']  = httpProxy\n",
    "os.environ['https_proxy'] = httpsProxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['engine\\r\\n', 'automotive\\r\\n', 'ECU']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path ,newline='') as f:\n",
    "    dataList = [line for line in f]\n",
    "f.close()\n",
    "dataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Keywords:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Keywords: \",len(dataList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipedia.set_lang('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storeResult(wikipage):\n",
    "    title=wikipage.title\n",
    "    summary=wikipage.summary.replace('\\n',' ')\n",
    "    appendList= title,summary\n",
    "    return appendList    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Anaconda3\\envs\\nlp\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "fileout=open('wiki_corpus_NN_summary.csv', 'w', encoding=\"utf8\",newline=\"\")\n",
    "writer = csv.writer(fileout,csv.QUOTE_MINIMAL)\n",
    "writer.writerow(['Title','Summary'])\n",
    "count = 0\n",
    "for data in dataList:\n",
    "    count = count + 1\n",
    "    if count % 5 == 0:\n",
    "        print(\"fetched summary upto %d keywords out of %d\"%(count ,len(dataList)))\n",
    "    suggestedList =[]\n",
    "    try:\n",
    "        wikipage = wikipedia.page(data)\n",
    "        writer.writerow(storeResult(wikipage))\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        suggestedList = e.options[:5]\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        suggestedList = wikipedia.search(data)\n",
    "    for lst in suggestedList[:5]:\n",
    "        try:\n",
    "            wikipage = wikipedia.page(lst)\n",
    "            writer.writerow(storeResult(wikipage))\n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            pass\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            pass\n",
    "fileout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getcategories(data):\n",
    "    categories = []\n",
    "    suggestedList =[]\n",
    "    try:\n",
    "        wikipage = wikipedia.page(data)\n",
    "        categories.append(wikipage.categories)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        suggestedList = e.options[:5]\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        suggestedList = wikipedia.search(data)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    for lst in suggestedList[:5]:\n",
    "        try:\n",
    "            wikipage = wikipedia.page(lst)\n",
    "            categories.append(wikipage.categories)\n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            pass\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            pass\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Anaconda3\\envs\\nlp\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "categories=[]\n",
    "count = 0\n",
    "for keyword in dataList:\n",
    "    count = count + 1\n",
    "    if count % 5 == 0:\n",
    "        print(\"fetched categories upto %d keywords out of %d\"%(count ,len(nouns)))\n",
    "    categories.extend(getcategories(keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_list_categories=[]\n",
    "for sublist in categories:\n",
    "    for item in sublist:\n",
    "            single_list_categories.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = list(set(single_list_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Articles to be expanded from January 2018', 'Eurozone', 'Articles with unsourced statements from March 2016', 'Wikipedia articles in need of updating from August 2017', 'Articles prone to spam from September 2015', 'Currency symbols', 'Articles with dead external links from December 2016', 'Articles with Kazakh-language external links', 'Multilateral relations of Russia', 'Fuel injection systems', 'Customs unions', 'All articles to be expanded', 'Embedded systems', 'Articles with unsourced statements from April 2010', '1266 establishments in France', 'CS1 Russian-language sources (ru)', 'Modern obsolete currencies', 'Automation', 'Power control', 'Organizations established in 2010', 'Articles with Russian-language external links', 'Commons category with local link different than on Wikidata', 'Articles with permanently dead external links', 'All articles with dead external links', 'Auto parts', 'Wikipedia articles needing clarification from June 2012', 'Articles with unsourced statements from December 2012', 'Cleanup tagged articles without a reason field from January 2012', 'Currencies of Europe', 'All Wikipedia articles needing clarification', 'Medieval currencies', 'All articles needing additional references', 'Articles with unsourced statements from November 2012', 'Commonwealth of Independent States', 'Articles using small message boxes', 'Articles with unsourced statements from February 2010', 'All Wikipedia articles in need of updating', 'International trade organizations', 'Engine control systems', 'Articles needing additional references from April 2016', 'Eurasia', 'Articles containing video clips', 'Economic history of the Ancien Régime', 'Pages using div col without cols and colwidth parameters', 'Articles needing cleanup from January 2012', 'Mass production', 'Webarchive template wayback links', 'All pages needing cleanup', 'Wikipedia articles needing clarification from February 2015', 'All articles with unsourced statements', 'ISO 4217', 'Use dmy dates from April 2011', 'Engine technology', '2010 in economics', 'Articles with unsourced statements from May 2011', 'CS1 uses Russian-language script (ru)', 'Articles needing additional references from March 2010', 'Articles with Belarusian-language external links', 'Post-Soviet alliances', 'Onboard computers', 'Engines', 'Articles with Armenian-language external links', 'Wikipedia pages needing cleanup from January 2012', 'Automotive industry', 'Wikipedia articles with GND identifiers', 'Coins of France', 'Eurasian Economic Union', 'Engine components', 'Heavy industry', 'Use dmy dates from April 2017']\n"
     ]
    }
   ],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Categories for 3 keywords: 70\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Categories for %d keywords: %d\"%(len(dataList),len(categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileout=open('wiki_corpus_NN_categories.csv', 'w', encoding=\"utf8\",newline=\"\")\n",
    "writer = csv.writer(fileout,csv.QUOTE_MINIMAL)\n",
    "writer.writerow([\"Categories\"])\n",
    "for data in categories:\n",
    "    writer.writerow([data])\n",
    "fileout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched summary upto 5 keywords out of 70\n",
      "fetched summary upto 10 keywords out of 70\n",
      "fetched summary upto 15 keywords out of 70\n",
      "fetched summary upto 20 keywords out of 70\n",
      "fetched summary upto 25 keywords out of 70\n",
      "fetched summary upto 30 keywords out of 70\n",
      "fetched summary upto 35 keywords out of 70\n",
      "fetched summary upto 40 keywords out of 70\n",
      "fetched summary upto 45 keywords out of 70\n",
      "fetched summary upto 50 keywords out of 70\n",
      "fetched summary upto 55 keywords out of 70\n",
      "fetched summary upto 60 keywords out of 70\n",
      "fetched summary upto 65 keywords out of 70\n",
      "fetched summary upto 70 keywords out of 70\n"
     ]
    }
   ],
   "source": [
    "fileout=open('wiki_corpus_NN_Category_summary.csv', 'w', encoding=\"utf8\",newline=\"\")\n",
    "writer = csv.writer(fileout,csv.QUOTE_MINIMAL)\n",
    "writer.writerow(['Title','Summary'])\n",
    "count = 0\n",
    "for data in categories:\n",
    "    count = count + 1\n",
    "    if count % 5 == 0:\n",
    "        print(\"fetched summary upto %d keywords out of %d\"%(count ,len(categories)))\n",
    "    suggestedList =[]\n",
    "    try:\n",
    "        wikipage = wikipedia.page(data)\n",
    "        writer.writerow(storeResult(wikipage))\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        suggestedList = e.options[:5]\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        suggestedList = wikipedia.search(data)\n",
    "    for lst in suggestedList[:5]:\n",
    "        try:\n",
    "            wikipage = wikipedia.page(lst)\n",
    "            writer.writerow(storeResult(wikipage))\n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            pass\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            pass\n",
    "fileout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
